---
title: "SING Bioinformatics Workshop - Part 1"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-expand: true
    embed-resources: true
    theme: litera
---






## Setup and Environment (10 min)


Below is a schematic of the structure of the workshop files and directories (folders)


```
sing_guam/
├── aligned/
│   ├── example.rs2231142_het.bam
│   ├── example.rs2231142_hom_alt.bam
│   └── example.rs2231142_hom_ref.bam
├── fastq/
│   ├── example.R1.fq
│   └── example.R2.fq
├── load_software.sh
├── ref/
│   └── human_g1k_v37.fasta
├── vcf/
│   ├── 1kgp.chrMT.vcf.gz
│   └── example_3_sample.vcf
├── workshop_part1.qmd
└── workshop_part2.qmd
```

To the left in the `Files` tab you will see the directory for `sing_guam`, click on that and then click on the `sing_guam.Rproj` file. This will set RStudio to work from this directory by default.

The first section of the workshop we're going to run some common bioinformatic tools. In a shared compute environment like the Otago OnDemand cluster, we need to load the software we want to use - to do that we have created a script that you will run in the `Terminal`:

1. ensure that you are using the `Terminal` tab

```{r}
#| eval: false

knitr::include_graphics("images/terminal_tab.png")
```


2. type the following into the `Terminal`

```{bash}
#| eval: false
cd sing_guam
source load_software.sh
```






### What is the 1000 Genomes Project (10 min)


The 1000 Genomes Project was an international consortium that was established in 2007 to provide a
comprehensive record of human genetic variation (Siva, 2008). The project consisted of three main
data phases. 
A pilot phase that whole-genome sequenced 179 individuals from four populations at low
coverage (2-4x), along with high coverage sequencing for two trios (mother, father, and child), and exon
targeted sequencing for 697 individuals from seven populations (1000 Genomes Project Consortium,
2010). 
The second main data phase provided sequencing data for 1092 individuals from 14 populations.
This sequencing data set was a combination of low coverage whole-genome and exon sequencing (1000
Genomes Project Consortium, 2012). 
The third main phase (Phase 3) was a dataset consisting of low
coverage whole-genome sequencing, deep exome sequencing, and dense SNP array genotyping for 2504
individuals from 26 populations (1000 Genomes Project Consortium, 2015). The 1000 Genomes data
set used in this workshop is from the Phase 3 release, and is publicly available from https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/.

The below shows the names of the sample populations, and their location.

```{r, fig.cap="1000 Genomes Sample Populations. By Taras K. Oleksyk, Vladimir Brukhin and Stephen J. O’Brien - Oleksyk TK, Brukhin V, O’Brien SJ. The Genome Russia project: closing the largest remaining omission on the world Genome map. GigaScience. 2015;4:53. doi:10.1186/s13742-015-0095-0. https://gigascience.biomedcentral.com/articles/10.1186/s13742-015-0095-0, CC BY 4.0, https://commons.wikimedia.org/w/index.php?curid=56695551"}
knitr::include_graphics("images/1000_Genomes_Project.svg.png")
```

We're going to make use of a small subset of this data to show examples of some data types and bioinformatic analyses that can be done.

## From the sequencer to analysis-ready data (10 min)

The data file that comes out of the sequencer will look like this:

> \@HWI-M01237:20:000000000-A5R87:1:1101:15995:1759 1:N:0:CCTAGGT
>
> TCTGAGGAGCTCTAATAACAAGCTCCATCTGCCTACGT
CAAACCGACTTAAAAGCACTCAAGATCGGAAGTGCA...
>
> +HWI-M01237:20:000000000-A5R87:1:1101:15995:1759 1:N:0:CCTAGGT
>
> FFFFBD?11GGGGCGGB3B110AFGC1FFH1EAF1B0B
A0/1AA//AAE1111101ABB0111ABA///?1B12...

We can have a quick look at these files:

```{bash}
#| eval: false

cd ~/sing_guam/fastq/
less -S fastq/example.R1.fastq
```

When you're done exploring, you can exit the viewer by pressing the <kbd>Q</kbd> key.

The file contain strings of DNA base pairs, identifiers, and encoded information about the quality of the sequencing run. The raw data undergoes quite a bit of processing before it can be used for analysis. The general pipeline looks like this:

1.  First, the quality of the data is assessed to check see how well the sequencing went and how much we can trust the sequencer’s calls.
2.  This can also include trimming of low-quality sequences or adapters (base pairs attached to the target DNA to aid in sequencing).
3.   You might end up sequencing “off-target” portions of DNA – these can be identified and/or removed by comparing your sequences to a large database.
4.   The sequence data is mapped to a reference.
5.  Some intermediary steps are performed like removing duplicate sequences, sorting, and indexing. 
6.   In areas where many reads are mapped and genetic variation is present, variants are called. These may be variation at a single base pair (SNP), or across many base pairs (structural variation).
7.   Variants are filtered and checked for quality, and possibly annotated and classified.
8.   Further analysis can now take place.

The pipeline can be represented in the figure below:


```{r}
#| out.width: "398px"
knitr::include_graphics("images/analysis-pipeline.png")
```


### Aligning reads (15-20min)

Once we have reads from our sequencer of choice, we want to have an idea about where they come from in the genome. To do this we need to have the genome sequence - in our case this will be what is known as the human reference genome - while it is called a "reference", it does not represent the full diversity of variation found in humans but enables us to _align_ our sequencing reads by looking for regions in the reference that match (with some tolerance) to the sequence of our reads. We use alignment over genome assembly if we have access to a good enough genomic sequence, as assembly is a very computationally intensive process.

Let’s take a closer look at how some portions of the pipeline will look from the bioinformatic perspective. We can begin by aligning some reads from the sequencer –we will align a subset of reads to a gene called _ABCG2_. This is a gene on the fourth human chromosome that creates a protein involved in transport across cell membranes.


The tool we use for aligning is the _burrows-wheeler aligner_ or `bwa`. It takes the fastq files and finds the best match of each read to the reference sequence, and then outputs a file that has read information from the fastq filess. The output also includes where in the genome the read mapped to, plus how well it mapped/matched. This is stored in a _sequence alignment map (SAM)_ file. The more efficient compressed binary version is the _bam_ file.


```{bash}
#| eval: false

cd ~/sing_guam/aligned

bwa mem ../ref/human_g1k_v37.fasta ../fastq/example.R1.fastq ../fastqexample.R2.fastq > example.sam
```

After this has finished, we can get some information about the quality of our mapping with the following tool:

`samtools flagstat mapped_reads.sam`

Let’s take a look at the output of mapping with a viewer:

`samtools tview ./mapped_reads.sam`

You can navigate with the arrow keys, or press `g` to jump to a particular region.

We see that each read has been placed somewhere along the reference, and there is a lot of overlap at each position. However, there is some variation present- some reads have one or more base pairs that are different from the reference sequence. When we have enough confidence that the different base pair is not an artifact of the sequencing, we call a variant – that is, identify a SNP – at that position.

We can call variants along the entire genome to get a sense of how much variation is present in the sample(s) of interest. Unfortunately, we won’t have time to do any variant calling ourselves today, but you can see the logic of how this works here.

When you're ready, exit the viewer by pressing the <kbd>Q</kbd> key.



```{bash}
#| eval: false

samtools tview rs2231142_hom_ref_example.bam -p 4:89052323 ../ref/human_g1k_v37.fasta
```

```{bash}
#| eval: false

samtools tview rs2231142_het_example.bam -p 4:89052323 ../ref/human_g1k_v37.fasta
```


```{bash}
#| eval: false

samtools tview rs2231142_hom_alt_example.bam -p 4:89052323 ../ref/human_g1k_v37.fasta
```

## VCF (20min)

The _Variant Call Format_ is a file format that is a summarisation of one or more aligned samples. Each row is a variant. This format has 8 main columns:

Name | Description
---|---
CHROM | Chromosome
POS | Position/coordinate on the chromsome
ID | Often a variant identifier
REF | Reference allele at the position
ALT | Alternative allele(s) observed at the position
QUAL | Phred score as to likelihood of variant
INFO | Contains overview statistics
FORMAT | Describes order of fields found in each sample column

Following these is a column per sample with the sample genotype and extra information at the sample level. Genotypes are represented as 0 = reference, 1 = alternate. For a diploid organism (such as humans) we get 0/0 for homozygous reference, 0/1 heterozygote, and 1/1 homozygous for the alternative allele.

It's important to note that reference and alternative are purely in relation to what allele the reference genome contained and does not tell you anything about the commonality of the allele population-wise.

### Bam -> VCF (10)

We aren't going to do the actual variant calling stage in this workshop as there are many different programs that are often used to accomplish this and is usually a multi-step process. Samtools, GATK, and Freebayes are the most common varint callers. The simplest method is a "pileup" where for each position of the genome a statistical model is applied to determine the likelihood of the genotype based on the number of reads for each allele present.


Instead we shall take a look at a VCF file that was made from the 3 example bams we just looked at but only for the position of the variant rs2231142.

```{bash}
#| eval: false

cd ~/sing_guam/vcf

less example.3_sample.vcf
```


Use the up/down arrow keys to scroll through this file.


### Haplotype tree prep (10 min)


Later on, we'll create our own phylogenetic tree for a population in the database. While we're looking through an example VCF, we'll run a subsetting function on the 1000 Genomes data to just one population. Subsetting is commonly applied to VCFs - you can select certain individuals, population groups, or even regions of the genome.

Choose a population code from the table below and run the following in `Terminal`:

```{bash}
#| eval: false

cd ~/sing_guam/vcf

grep "PEL" 1kgp-phase3-samples.txt | cut -f1 > samples-of-interest.txt
```

NB: change `PEL` to any of the codes listed in the table below to select a particular population:

|     |                                                               |
|:---:|---------------------------------------------------------------|
| ACB | African Caribbean in Barbados                                 |
| ASW | African Ancestry in Southwest USA                             |
| CDX | Chinese Dai in Xishuangbanna, China                           |
| CEU | Utah residents with ancestry from northern and western Europe |
| CHB | Han Chinese in Beijing, China                                 |
| CHS | Han Chinese South, China                                      |
| CLM | Colombian in Medellin, Colombia                               |
| FIN | Finnish in Finland                                            |
| GBR | British From England and Scotland, UK                         |
| GIH | Gujarati Indians in Houston, Texas, USA                       |
| IBS | Iberian populations in Spain                                  |
| JPT | Japanese in Tokyo, Japan                                      |
| KHV | Kinh in Ho Chi Minh City, Vietnam                             |
| LWK | Luhya in Webuye, Kenya                                        |
| MXL | Mexican Ancestry in Los Angeles, California, USA              |
| PEL | Peruvian in Lima, Peru                                        |
| PUR | Puerto Rican in Puerto Rico                                   |
| TSI | Toscani in Italia                                             |
| YRI | Yoruba in Ibadan, Nigeria                                     |

: ^Table\ from\ Diroma\ et\ al.,\ 2014\ BMC\ Genomics\ (<https://doi.org/10.1186%2F1471-2164-15-S3-S2>)^

Again in `Terminal`, run the following:

```{bash}
#| eval: false

bcftools view --samples-file samples-of-interest.txt --output pop.chrMT.vcf.gz 1kgp.chrMT.vcf.gz
```

We'll come back to this file a bit later on, but you can take a look at your subsetted VCF file in the same way we did before with:

```{bash}
#| eval: false

less 1kgp.chrMT.vcf.gz
```


## Variants – what can we do with them? (20min)

Once you’ve mapped reads and called your variants, what does one do with them? How does one use this data? Let’s assume we’re working with human data in a medical genetics context. We’ve identified a variant that seems to be associated a particular clinical outcome. We might be interested to know what the allele frequency of this variant is (i.e., how common is it?) Let’s assess the frequency of a variant called rs2231142, which is located in the ABCG2 gene we mapped earlier:

There are many online databases dedicated to recording genomic variation and providing more context or information about what is known about the variant. One such online database is [ensembl](https://ensembl.org). We can use this database to find out some more about our variant rs2231142 at [ensembl](https://ensembl.org). We'll use the links below to find out more:

- [Genomic context](https://grch37.ensembl.org/Homo_sapiens/Location/View?db=core;r=4:89051823-89052823;v=rs2231142;vdb=variation;vf=253533488)
- [Variant info](https://grch37.ensembl.org/Homo_sapiens/Variation/Explore?r=4:89051823-89052823;v=rs2231142;vdb=variation;vf=253533488)
- [Population frequencies](https://grch37.ensembl.org/Homo_sapiens/Variation/Population?db=core;r=4:89051823-89052823;v=rs2231142;vdb=variation;vf=253533488)
- [Phenotypes](https://grch37.ensembl.org/Homo_sapiens/Variation/Phenotype?db=core;r=4:89051823-89052823;v=rs2231142;vdb=variation;vf=253533488)


## Nanopore break (10/15 min)



# Appendix

## BASH quick reference

Navigation

- `pwd` tells you where you are in the file system

- `ls` will show you the names of files and directories where you are
  - `ls -F` will append a `/` onto the end of the names that are directories

- `cd` by itself will take you back to your home directory (`~/`)

- `cd directory_name` will place you in that directory, e.g `cd ~/sing_guam` will move you to the workshop area if you got mis-placed

- `./` infront of a file or directory means bash will look in the current directory (that shown by `pwd`)

- `../` infront of a file or directory means the parent directory (look one directory "up"), `../../` means look "2-up", etc.

- <kbd>TAB</kbd> for completing program and file names. As you are typing make frequent use of hitting the <kbd>TAB</kbd> key as it will try to auto-complete file names/paths which will prevent typos.

Other utilities:

- `grep` is a commandline program that can search for patters in files
- `cut` lets you choose which columns you want to include from a file by adding `-f <column numbers>`

